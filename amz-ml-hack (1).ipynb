{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13368349,"sourceType":"datasetVersion","datasetId":8480556},{"sourceId":13368987,"sourceType":"datasetVersion","datasetId":8481071},{"sourceId":13369438,"sourceType":"datasetVersion","datasetId":8481430},{"sourceId":13371910,"sourceType":"datasetVersion","datasetId":8483371}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestRegressor, StackingRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sentence_transformers import SentenceTransformer\nimport joblib\nfrom tqdm import tqdm\ntqdm.pandas()\n\n\n# ---------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:31:16.117468Z","iopub.execute_input":"2025-10-13T17:31:16.118066Z","iopub.status.idle":"2025-10-13T17:31:27.510806Z","shell.execute_reply.started":"2025-10-13T17:31:16.118042Z","shell.execute_reply":"2025-10-13T17:31:27.510189Z"}},"outputs":[{"name":"stderr","text":"2025-10-13 17:31:24.198784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760376684.224109     198 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760376684.231536     198 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =========================================================\n# 2Ô∏è‚É£ SMAPE Metric\n# =========================================================\ndef smape(y_true, y_pred):\n    return 100/len(y_true) * np.sum(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n\n# =========================================================\n# 3Ô∏è‚É£ Load Cleaned Data\n# =========================================================\ntrain = pd.read_csv(\"/kaggle/input/cleaned-train-csv/cleaned_train.csv\")\ntest = pd.read_csv(\"/kaggle/input/cleaned-test-csv-final/data_cleaned_test.csv\")\n\nprint(\"‚úÖ Train shape:\", train.shape)\nprint(\"‚úÖ Test shape:\", test.shape)\n\n# Target variable (log1p)\ny = np.log1p(train[\"price\"].values)\nX_train = train.drop(columns=[\"price\", \"prod_id\"])\nX_test = test.drop(columns=[\"prod_id\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T17:32:48.995083Z","iopub.execute_input":"2025-10-13T17:32:48.996135Z","iopub.status.idle":"2025-10-13T17:32:51.588175Z","shell.execute_reply.started":"2025-10-13T17:32:48.996110Z","shell.execute_reply":"2025-10-13T17:32:51.587046Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Train shape: (75000, 16)\n‚úÖ Test shape: (75004, 10)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_198/3911047952.py:11: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n  test = pd.read_csv(\"/kaggle/input/cleaned-test-csv-final/data_cleaned_test.csv\")\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =========================================================\n# 3Ô∏è‚É£ Text Embeddings\n# =========================================================\nMODEL_PATH = \"/kaggle/input/minilm-trans/miniLM\"\nencoder = SentenceTransformer(MODEL_PATH)\n\nprint(\"üî§ Generating MiniLM text embeddings...\")\ntrain_text = (train[\"item_name\"].fillna('') + \". \" + train[\"prod_desc\"].fillna('')).tolist()\ntest_text = (test[\"item_name\"].fillna('') + \". \" + test[\"prod_desc\"].fillna('')).tolist()\n\nX_train_emb = encoder.encode(train_text, show_progress_bar=True, convert_to_numpy=True, batch_size=64)\nX_test_emb = encoder.encode(test_text, show_progress_bar=True, convert_to_numpy=True, batch_size=64)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:08:11.585663Z","iopub.execute_input":"2025-10-13T18:08:11.586178Z","iopub.status.idle":"2025-10-13T18:12:41.668006Z","shell.execute_reply.started":"2025-10-13T18:08:11.586154Z","shell.execute_reply":"2025-10-13T18:12:41.667181Z"}},"outputs":[{"name":"stdout","text":"üî§ Generating MiniLM text embeddings...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bf26724231d47e78efa38039089bb46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1172 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d3a2435791e4361a1092e4c6e1a9c34"}},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"\n# =========================================================\n# 4Ô∏è‚É£ Numeric Features + Scaling\n# =========================================================\nnum_features = [\"pack_count\", \"num_bullets\", \"desc_len\", \"title_len\", \"has_pack_info\"]\n\nscaler = StandardScaler()\nX_train_num = scaler.fit_transform(train[num_features])\nX_test_num = scaler.transform(test[num_features])\n\nX_train_final = np.hstack([X_train_emb, X_train_num])\nX_test_final = np.hstack([X_test_emb, X_test_num])\n\n# =========================================================\n# 5Ô∏è‚É£ Ensemble Model (Optimized for Speed)\n# =========================================================\nlgb = LGBMRegressor(\n    n_estimators=400,\n    learning_rate=0.05,\n    max_depth=8,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    device=\"gpu\",  # use GPU on Kaggle\n    random_state=42\n)\n\ncat = CatBoostRegressor(\n    iterations=400,\n    learning_rate=0.05,\n    depth=8,\n    task_type=\"GPU\",  # use GPU on Kaggle\n    verbose=0,\n    random_state=42\n)\n\nensemble = StackingRegressor(\n    estimators=[(\"lgb\", lgb), (\"cat\", cat)],\n    final_estimator=LGBMRegressor(n_estimators=300, learning_rate=0.05, random_state=42, device=\"gpu\"),\n    n_jobs=1  # ‚ö† important\n)\n# =========================================================\n# 6Ô∏è‚É£ 3-Fold Cross-Validation\n# =========================================================\nkf = KFold(n_splits=3, shuffle=True, random_state=42)\ncv_scores = []\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X_train_final)):\n    print(f\"\\nüîπ Fold {fold+1}\")\n    X_tr, X_val = X_train_final[train_idx], X_train_final[val_idx]\n    y_tr, y_val = y[train_idx], y[val_idx]\n\n    ensemble.fit(X_tr, y_tr)\n    y_val_pred = ensemble.predict(X_val)\n\n    y_val_pred_exp = np.expm1(y_val_pred)\n    y_val_exp = np.expm1(y_val)\n    fold_smape = smape(y_val_exp, y_val_pred_exp)\n    print(f\"SMAPE = {fold_smape:.4f}%\")\n    cv_scores.append(fold_smape)\n\nprint(\"\\nüìä Mean CV SMAPE:\", np.mean(cv_scores))\n\n# =========================================================\n# 7Ô∏è‚É£ Train on Full Data & Predict Test\n# =========================================================\nprint(\"\\nüöÄ Training final model on full data...\")\nensemble.fit(X_train_final, y)\ny_test_pred = ensemble.predict(X_test_final)\ny_test_pred = np.expm1(y_test_pred)\ny_test_pred = np.clip(y_test_pred, 0.01, None)\n\nsubmission = pd.DataFrame({\n    \"sample_id\": test[\"prod_id\"].values,\n    \"price\": y_test_pred\n})\nsubmission.to_csv(\"/kaggle/working/submission_fast.csv\", index=False)\n\nprint(\"üíæ Submission saved ‚Üí /kaggle/working/submission_fast.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-13T18:12:59.106250Z","iopub.execute_input":"2025-10-13T18:12:59.106933Z","iopub.status.idle":"2025-10-13T18:27:50.149557Z","shell.execute_reply.started":"2025-10-13T18:12:59.106905Z","shell.execute_reply":"2025-10-13T18:27:50.148797Z"}},"outputs":[{"name":"stdout","text":"\nüîπ Fold 1\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98510\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (18.50 MB) transferred to GPU in 0.019597 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741552\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98506\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.016006 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.740792\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98504\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015442 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.744771\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98505\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015989 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.739081\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98505\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015331 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.742377\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98506\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015984 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.740737\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 510\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 2\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 2 dense feature groups (0.19 MB) transferred to GPU in 0.000626 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741552\nSMAPE = 60.3590%\n\nüîπ Fold 2\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98516\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (18.50 MB) transferred to GPU in 0.019030 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741229\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98512\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.016979 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741228\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98510\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.016618 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.742944\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98511\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.018820 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.739711\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98513\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015737 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.742563\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98512\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.015887 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.739700\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 510\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 2\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 2 dense feature groups (0.19 MB) transferred to GPU in 0.000858 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741229\nSMAPE = 59.9367%\n\nüîπ Fold 3\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98514\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (18.50 MB) transferred to GPU in 0.021881 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.734871\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98513\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.017300 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.733923\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98510\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.018851 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.736293\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98511\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.016209 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.733019\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98511\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.019180 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.737104\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98511\n[LightGBM] [Info] Number of data points in the train set: 40000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (14.80 MB) transferred to GPU in 0.016100 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.734018\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 510\n[LightGBM] [Info] Number of data points in the train set: 50000, number of used features: 2\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 2 dense feature groups (0.19 MB) transferred to GPU in 0.000843 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 2.734871\nSMAPE = 59.9693%\n\nüìä Mean CV SMAPE: 60.088354963205724\n\nüöÄ Training final model on full data...\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98519\n[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (27.75 MB) transferred to GPU in 0.028875 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.739217\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98517\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022814 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.738583\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98517\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.026009 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.741352\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98516\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.030852 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.737225\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98519\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.023347 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.740552\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 98515\n[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 389\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 387 dense feature groups (22.20 MB) transferred to GPU in 0.022839 secs. 1 sparse feature groups\n[LightGBM] [Info] Start training from score 2.738375\n[LightGBM] [Info] This is the GPU trainer!!\n[LightGBM] [Info] Total Bins 510\n[LightGBM] [Info] Number of data points in the train set: 75000, number of used features: 2\n[LightGBM] [Info] Using GPU Device: Tesla T4, Vendor: NVIDIA Corporation\n[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n[LightGBM] [Info] GPU programs have been built\n[LightGBM] [Info] Size of histogram bin entry: 8\n[LightGBM] [Info] 2 dense feature groups (0.29 MB) transferred to GPU in 0.000650 secs. 0 sparse feature groups\n[LightGBM] [Info] Start training from score 2.739217\nüíæ Submission saved ‚Üí /kaggle/working/submission_fast.csv\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# -------------------------\n# Paths\n# -------------------------\nDATA_PATH = \"/kaggle/input/cleaned-train-csv/cleaned_train.csv\"\nTEST_PATH = \"/kaggle/input/cleaned-test-csv-final/data_cleaned_test.csv\"  # your test CSV\nMODEL_PATH = \"/kaggle/input/minilm-trans/miniLM\"  # local MiniLM model\nMODEL_OUTPUT_PATH = \"value_predictor_lgbm.pkl\"\nENCODER_PATH = \"text_encoder.pkl\"\nSUBMISSION_PATH = \"/kaggle/working/submission.csv\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =============================================\n# Helper Functions\n# =============================================\n\n# Load CSV\ndef load_data(path):\n    print(f\"üìÇ Loading dataset: {path}\")\n    df = pd.read_csv(path)\n    print(f\"‚úÖ Loaded {len(df)} rows and {len(df.columns)} columns\")\n    return df\n\ndef prepare_text_features(df):\n    \"\"\"Combine item_name, prod_desc, and bullet_points into one text column.\"\"\"\n    text_cols = [\"item_name\", \"prod_desc\", \"bullet_points\"]\n    for col in text_cols:\n        df[col] = df[col].fillna(\"\")\n\n    df[\"combined_text\"] = (\n        df[\"item_name\"] + \". \" +\n        df[\"prod_desc\"] + \". \" +\n        df[\"bullet_points\"]\n    ).str.strip()\n\n    return df\n\n# Encode text using MiniLM embeddings (batch for speed)\ndef encode_text(df, model_path=MODEL_PATH, batch_size=64):\n    print(f\"üî§ Loading SentenceTransformer model from: {model_path}\")\n    encoder = SentenceTransformer(model_path, local_files_only=True, trust_remote_code=False)\n\n    texts = df[\"combined_text\"].tolist()\n    embeddings = []\n    for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding text\"):\n        batch = texts[i:i+batch_size]\n        batch_emb = encoder.encode(batch, show_progress_bar=False)\n        embeddings.append(batch_emb)\n    embeddings = np.vstack(embeddings)\n    return embeddings, encoder\n\n# Train LightGBM regression model\ndef train_model(X, y):\n    print(\"üöÄ Training LightGBM model...\")\n    model = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"regressor\", LGBMRegressor(\n            n_estimators=800,\n            learning_rate=0.05,\n            max_depth=10,\n            subsample=0.9,\n            colsample_bytree=0.9,\n            reg_lambda=0.1,\n            random_state=92,\n            n_jobs=-1\n        ))\n    ])\n    model.fit(X, y)\n    return model\n\n# Evaluate regression model\ndef evaluate(model, X_test, y_test):\n    y_pred = model.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    print(\"\\nüìä Evaluation Results:\")\n    print(f\"MAE  = {mae:.4f}\")\n    print(f\"RMSE = {rmse:.4f}\")\n    print(f\"R¬≤   = {r2:.4f}\")\n\n# Save model and encoder\ndef save_model(model, encoder):\n    joblib.dump(model, MODEL_OUTPUT_PATH)\n    joblib.dump(encoder, ENCODER_PATH)\n    print(f\"üíæ Saved model ‚Üí {MODEL_OUTPUT_PATH}\")\n    print(f\"üíæ Saved encoder ‚Üí {ENCODER_PATH}\")\n\n# SMAPE metric for submission evaluation\ndef smape(y_true, y_pred):\n    return 100 * np.mean(np.abs(y_pred - y_true) / ((np.abs(y_true) + np.abs(y_pred)) / 2))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ndf = load_data(\"/kaggle/input/cleaned-train-csv/cleaned_train.csv\")\n\n# Prepare text features\ndf = prepare_text_features(df)\n\n# Target\ny = df[\"price\"].values  # predicting 'price'\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode combined text into embeddings\nX, encoder = encode_text(df, model_path=MODEL_PATH, batch_size=64)\nprint(f\"‚úÖ Text embeddings shape: {X.shape}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split data for evaluation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\nprint(f\"Training samples: {X_train.shape[0]}, Validation samples: {X_val.shape[0]}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = train_model(X_train, y_train)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"evaluate(model, X_val, y_val)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def smape(y_true, y_pred):\n    \"\"\"Compute Symmetric Mean Absolute Percentage Error (SMAPE).\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n    diff = np.abs(y_pred - y_true) / denominator\n    diff[denominator == 0] = 0  # handle zero division\n    return np.mean(diff) * 100\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict on validation set\ny_val_pred = model.predict(X_val)\n\n# Compute SMAPE\nval_smape = smape(y_val, y_val_pred)\nprint(f\"üìä Validation SMAPE: {val_smape:.4f}%\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_model(model, encoder)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def prepare_test_text(df):\n    \"\"\"Use catalog_content column from test set as combined_text.\"\"\"\n    df[\"catalog_content\"] = df[\"catalog_content\"].fillna(\"\")\n    df[\"combined_text\"] = df[\"catalog_content\"].str.strip()\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load test data\ndf_test = pd.read_csv(\"/kaggle/input/cleaned-test-csv/test.csv\")\n\n# Prepare text\ndf_test = prepare_test_text(df_test)\n\n# df_test[\"combined_text\"] is now ready for encoding\n\n# Encode test set text\nX_test, _ = encode_text(df_test, model_path=MODEL_PATH, batch_size=64)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predict prices\ny_pred = model.predict(X_test)\n\n# Ensure all predicted prices are positive\ny_pred = np.maximum(y_pred, 0.01)\n\n# Prepare submission\nsubmission = pd.DataFrame({\n    \"sample_id\": df_test[\"sample_id\"],\n    \"price\": y_pred\n})\n\n# Save submission\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"‚úÖ Submission saved ‚Üí {SUBMISSION_PATH}\")\nsubmission.head()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}